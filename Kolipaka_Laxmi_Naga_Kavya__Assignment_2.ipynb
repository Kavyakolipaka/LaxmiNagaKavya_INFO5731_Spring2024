{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Wednesday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (40 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from G2 or Capterra.\n",
        "\n",
        "(4) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the Densho Digital Repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jDyTKYs-yGit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3150ddf5-e8ff-46d3-ab82-859e7e526656"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected 25 reviews for https://www.imdb.com/title/tt6791350/reviews/?ref_=tt_ql_2\n",
            "Collected 50 reviews for https://www.imdb.com/title/tt6791350/reviews/?ref_=tt_ql_2\n",
            "Collected 75 reviews for https://www.imdb.com/title/tt6791350/reviews/?ref_=tt_ql_2\n",
            "Collected 100 reviews for https://www.imdb.com/title/tt6791350/reviews/?ref_=tt_ql_2\n",
            "Collected 125 reviews for https://www.imdb.com/title/tt6791350/reviews/?ref_=tt_ql_2\n",
            "Collected 150 reviews for https://www.imdb.com/title/tt6791350/reviews/?ref_=tt_ql_2\n",
            "Collected 175 reviews for https://www.imdb.com/title/tt6791350/reviews/?ref_=tt_ql_2\n",
            "Collected 200 reviews for https://www.imdb.com/title/tt6791350/reviews/?ref_=tt_ql_2\n",
            "Collected 225 reviews for https://www.imdb.com/title/tt6791350/reviews/?ref_=tt_ql_2\n",
            "Collected 250 reviews for https://www.imdb.com/title/tt6791350/reviews/?ref_=tt_ql_2\n",
            "Collected 275 reviews for https://www.imdb.com/title/tt6791350/reviews/?ref_=tt_ql_2\n",
            "Collected 300 reviews for https://www.imdb.com/title/tt6791350/reviews/?ref_=tt_ql_2\n",
            "Collected 325 reviews for https://www.imdb.com/title/tt6791350/reviews/?ref_=tt_ql_2\n",
            "Collected 350 reviews for https://www.imdb.com/title/tt6791350/reviews/?ref_=tt_ql_2\n",
            "Collected 375 reviews for https://www.imdb.com/title/tt6791350/reviews/?ref_=tt_ql_2\n",
            "Collected 400 reviews for https://www.imdb.com/title/tt6791350/reviews/?ref_=tt_ql_2\n",
            "Collected 425 reviews for https://www.imdb.com/title/tt6791350/reviews/?ref_=tt_ql_2\n",
            "Collected 450 reviews for https://www.imdb.com/title/tt6791350/reviews/?ref_=tt_ql_2\n",
            "Collected 475 reviews for https://www.imdb.com/title/tt6791350/reviews/?ref_=tt_ql_2\n",
            "Collected 500 reviews for https://www.imdb.com/title/tt6791350/reviews/?ref_=tt_ql_2\n",
            "Collected 525 reviews for https://www.imdb.com/title/tt6791350/reviews/?ref_=tt_ql_2\n",
            "Collected 550 reviews for https://www.imdb.com/title/tt6791350/reviews/?ref_=tt_ql_2\n",
            "Collected 575 reviews for https://www.imdb.com/title/tt6791350/reviews/?ref_=tt_ql_2\n",
            "Collected 600 reviews for https://www.imdb.com/title/tt6791350/reviews/?ref_=tt_ql_2\n",
            "Collected 625 reviews for https://www.imdb.com/title/tt6791350/reviews/?ref_=tt_ql_2\n",
            "Collected 650 reviews for https://www.imdb.com/title/tt6791350/reviews/?ref_=tt_ql_2\n",
            "Collected 675 reviews for https://www.imdb.com/title/tt6791350/reviews/?ref_=tt_ql_2\n",
            "Collected 700 reviews for https://www.imdb.com/title/tt6791350/reviews/?ref_=tt_ql_2\n",
            "Collected 725 reviews for https://www.imdb.com/title/tt6791350/reviews/?ref_=tt_ql_2\n",
            "Collected 750 reviews for https://www.imdb.com/title/tt6791350/reviews/?ref_=tt_ql_2\n",
            "Collected 775 reviews for https://www.imdb.com/title/tt6791350/reviews/?ref_=tt_ql_2\n",
            "Collected 800 reviews for https://www.imdb.com/title/tt6791350/reviews/?ref_=tt_ql_2\n",
            "Collected 825 reviews for https://www.imdb.com/title/tt6791350/reviews/?ref_=tt_ql_2\n",
            "Collected 850 reviews for https://www.imdb.com/title/tt6791350/reviews/?ref_=tt_ql_2\n",
            "Collected 875 reviews for https://www.imdb.com/title/tt6791350/reviews/?ref_=tt_ql_2\n",
            "Collected 900 reviews for https://www.imdb.com/title/tt6791350/reviews/?ref_=tt_ql_2\n",
            "Collected 925 reviews for https://www.imdb.com/title/tt6791350/reviews/?ref_=tt_ql_2\n",
            "Collected 950 reviews for https://www.imdb.com/title/tt6791350/reviews/?ref_=tt_ql_2\n",
            "Collected 975 reviews for https://www.imdb.com/title/tt6791350/reviews/?ref_=tt_ql_2\n",
            "Collected 1000 reviews for https://www.imdb.com/title/tt6791350/reviews/?ref_=tt_ql_2\n",
            "Successfully saved \n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "def scrape_imdb_reviews(movie_urls):\n",
        "    all_reviews = []\n",
        "    for url in movie_urls:\n",
        "        reviews = []\n",
        "        page = 1\n",
        "        while len(reviews) < 1000:\n",
        "            response = requests.get(url + f'&start={page * 10}')\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.content, 'html.parser')\n",
        "                review_containers = soup.find_all('div', class_='text show-more__control')\n",
        "                if not review_containers:\n",
        "                    break\n",
        "                for container in review_containers:\n",
        "                    review_text = container.text.strip()\n",
        "                    reviews.append(review_text)\n",
        "                    if len(reviews) >= 1000:\n",
        "                        break\n",
        "                print(f\"Collected {len(reviews)} reviews for {url}\")\n",
        "                page += 1\n",
        "            else:\n",
        "                print(f\"Failed to retrieve data from {url}\")\n",
        "                break\n",
        "        all_reviews.extend(reviews)\n",
        "    return all_reviews\n",
        "\n",
        "\n",
        "def get_movie_title(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        title = soup.find('meta', property='og:title')['content']\n",
        "        return title\n",
        "    else:\n",
        "        print(f\"Failed to retrieve data from {url}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def save_reviews_to_csv(reviews, filename, movie_name):\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Movie Name', 'Review'])\n",
        "        writer.writerows([[movie_name, review] for review in reviews])\n",
        "\n",
        "\n",
        "def main():\n",
        "    movie_url = 'https://www.imdb.com/title/tt6791350/reviews/?ref_=tt_ql_2'  # Replace with the IMDb URL of your chosen movie\n",
        "    movie_name = get_movie_title(movie_url)\n",
        "\n",
        "    if movie_name:\n",
        "        reviews = scrape_imdb_reviews([movie_url])\n",
        "        if len(reviews) >= 1000:\n",
        "            save_reviews_to_csv(reviews[:1000], 'movie_reviews.csv', movie_name)\n",
        "            print(f\"Successfully saved \")\n",
        "        else:\n",
        "            print(f\"Failed to collect 1000 reviews \", len(reviews), \"reviews.\")\n",
        "    else:\n",
        "        print(\"Failed to get movie name.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (30 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5QX6bJjGWXY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcc8d3fe-d2d2-490a-9377-fa9b6e0aaaef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                     Movie Name  \\\n",
            "0  Guardians of the Galaxy Vol. 3 (2023) - IMDb   \n",
            "1  Guardians of the Galaxy Vol. 3 (2023) - IMDb   \n",
            "2  Guardians of the Galaxy Vol. 3 (2023) - IMDb   \n",
            "3  Guardians of the Galaxy Vol. 3 (2023) - IMDb   \n",
            "4  Guardians of the Galaxy Vol. 3 (2023) - IMDb   \n",
            "\n",
            "                                              Review  \\\n",
            "0  Guardians of the Galaxy Volume 3 is chaotic, w...   \n",
            "1  Having sat through some phase 4 films that fai...   \n",
            "2  Up to this point, there has been one trilogy i...   \n",
            "3  \"There is no God. That's why I stepped in.\" I ...   \n",
            "4  Firstly Adam warlock's intro was marvelous, wh...   \n",
            "\n",
            "                                      Cleaned Review  \n",
            "0  guardian galaxi volum chaotic weird oftentim r...  \n",
            "1  sat phase film fail inspir guardian feel like ...  \n",
            "2  point one trilog mcu excel start finish time a...  \n",
            "3  god that step admit one best line ever spoken ...  \n",
            "4  firstli adam warlock intro marvel made hate ka...  \n",
            "                                       Movie Name  \\\n",
            "0    Guardians of the Galaxy Vol. 3 (2023) - IMDb   \n",
            "1    Guardians of the Galaxy Vol. 3 (2023) - IMDb   \n",
            "2    Guardians of the Galaxy Vol. 3 (2023) - IMDb   \n",
            "3    Guardians of the Galaxy Vol. 3 (2023) - IMDb   \n",
            "4    Guardians of the Galaxy Vol. 3 (2023) - IMDb   \n",
            "..                                            ...   \n",
            "995  Guardians of the Galaxy Vol. 3 (2023) - IMDb   \n",
            "996  Guardians of the Galaxy Vol. 3 (2023) - IMDb   \n",
            "997  Guardians of the Galaxy Vol. 3 (2023) - IMDb   \n",
            "998  Guardians of the Galaxy Vol. 3 (2023) - IMDb   \n",
            "999  Guardians of the Galaxy Vol. 3 (2023) - IMDb   \n",
            "\n",
            "                                                Review  \\\n",
            "0    Guardians of the Galaxy Volume 3 is chaotic, w...   \n",
            "1    Having sat through some phase 4 films that fai...   \n",
            "2    Up to this point, there has been one trilogy i...   \n",
            "3    \"There is no God. That's why I stepped in.\" I ...   \n",
            "4    Firstly Adam warlock's intro was marvelous, wh...   \n",
            "..                                                 ...   \n",
            "995  James Gunn is one of the few filmmakers who ha...   \n",
            "996  Struggling to find the right word(s) to sum th...   \n",
            "997  The MCU and Disney as a whole have had a large...   \n",
            "998  It's not bad. It's much more mature than previ...   \n",
            "999  One word comes to mind: contrivance.This film ...   \n",
            "\n",
            "                                        Cleaned Review  \n",
            "0    guardian galaxi volum chaotic weird oftentim r...  \n",
            "1    sat phase film fail inspir guardian feel like ...  \n",
            "2    point one trilog mcu excel start finish time a...  \n",
            "3    god that step admit one best line ever spoken ...  \n",
            "4    firstli adam warlock intro marvel made hate ka...  \n",
            "..                                                 ...  \n",
            "995  jame gunn one filmmak oper massiv system bigge...  \n",
            "996  struggl find right word sum underwhelm decent ...  \n",
            "997  mcu disney whole larg amount dud recent year f...  \n",
            "998  bad much matur previou guardian unfortun matur...  \n",
            "999  one word come mind contrivancethi film one gig...  \n",
            "\n",
            "[1000 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download NLTK resources if not already downloaded\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def clean_text(input_text):\n",
        "    # Remove noise (special characters and punctuations)\n",
        "    cleaned_text = re.sub(r'[^\\w\\s]', '', input_text)\n",
        "\n",
        "    # Remove numbers\n",
        "    cleaned_text = re.sub(r'\\d+', '', cleaned_text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(cleaned_text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word.lower() for word in words if word.lower() not in stop_words]\n",
        "\n",
        "    # Lowercase all texts\n",
        "    words = [word.lower() for word in words]\n",
        "\n",
        "    # Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    # Join the cleaned words back into a sentence\n",
        "    cleaned_text = ' '.join(words)\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "def clean_and_save_data(input_csv_filename):\n",
        "    # Read the CSV file\n",
        "    df = pd.read_csv(input_csv_filename)\n",
        "\n",
        "    # Apply the cleaning function to the 'Review' column and create a new 'Cleaned Review' column\n",
        "    df['Cleaned Review'] = df['Review'].apply(clean_text)\n",
        "\n",
        "    # Save the cleaned data to the same CSV file\n",
        "    df.to_csv(input_csv_filename, index=False)\n",
        "\n",
        "    # Print the head of the DataFrame\n",
        "    print(df.head())\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "input_csv_file = 'movie_reviews.csv'\n",
        "cleaned_df = clean_and_save_data(input_csv_file)\n",
        "\n",
        "\n",
        "print(cleaned_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (30 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Y0oOSlsOS0cq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b68b8450-948d-4a3e-9b2a-76acd8fe41c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "guardian galaxi volum chaotic weird oftentim ridicul also full heart emot great themesi must say best marvel movi sinc endgam that necessarili hard though need surpass way home amaz high moment lazi other marvel desper need hit theyv final got ithighlightseveri member crew got time shine rocket definit one stood though see friend die wail pain grief made bawl high evolutionari mock man heavi stuff proce rip face two friend shot well thu rocket traumat pastim revealedchukwudi iwuji fantast villain certain point downright terrifi realli like line god that took place convinc villainth moment starlord scream agoni rocket live moment reson lost mani peopl close cant stand lose anyon elsegamora get lot charact develop im glad didnt kiss quill end would felt littl cheapdrax manti nebula adam warlock other also got moment well im surpris nobodi die honeston problem movi humor lot time good passabl time undercut realli emot scenesi could say lot review alreadi long\n",
            "\n",
            "POS Tags (NLTK):\n",
            "[('guardian', 'JJ'), ('galaxi', 'NN'), ('volum', 'NN'), ('chaotic', 'JJ'), ('weird', 'NN'), ('oftentim', 'NN'), ('ridicul', 'NN'), ('also', 'RB'), ('full', 'JJ'), ('heart', 'NN'), ('emot', 'VBZ'), ('great', 'JJ'), ('themesi', 'NN'), ('must', 'MD'), ('say', 'VB'), ('best', 'JJS'), ('marvel', 'NN'), ('movi', 'NN'), ('sinc', 'NN'), ('endgam', 'NN'), ('that', 'WDT'), ('necessarili', 'RB'), ('hard', 'RB'), ('though', 'IN'), ('need', 'JJ'), ('surpass', 'NN'), ('way', 'NN'), ('home', 'NN'), ('amaz', 'VBP'), ('high', 'JJ'), ('moment', 'NN'), ('lazi', 'VBZ'), ('other', 'JJ'), ('marvel', 'NN'), ('desper', 'IN'), ('need', 'NN'), ('hit', 'VBN'), ('theyv', 'JJ'), ('final', 'JJ'), ('got', 'VBD'), ('ithighlightseveri', 'JJ'), ('member', 'NN'), ('crew', 'NN'), ('got', 'VBD'), ('time', 'NN'), ('shine', 'NN'), ('rocket', 'NN'), ('definit', 'NN'), ('one', 'CD'), ('stood', 'NN'), ('though', 'IN'), ('see', 'NN'), ('friend', 'NN'), ('die', 'VBP'), ('wail', 'NN'), ('pain', 'NN'), ('grief', 'NN'), ('made', 'VBN'), ('bawl', 'RB'), ('high', 'JJ'), ('evolutionari', 'NN'), ('mock', 'NN'), ('man', 'NN'), ('heavi', 'VBZ'), ('stuff', 'JJ'), ('proce', 'NN'), ('rip', 'NN'), ('face', 'NN'), ('two', 'CD'), ('friend', 'NN'), ('shot', 'NN'), ('well', 'RB'), ('thu', 'RB'), ('rocket', 'NN'), ('traumat', 'NN'), ('pastim', 'NN'), ('revealedchukwudi', 'NN'), ('iwuji', 'NN'), ('fantast', 'NN'), ('villain', 'NN'), ('certain', 'JJ'), ('point', 'NN'), ('downright', 'NN'), ('terrifi', 'NN'), ('realli', 'VBP'), ('like', 'IN'), ('line', 'NN'), ('god', 'NN'), ('that', 'WDT'), ('took', 'VBD'), ('place', 'NN'), ('convinc', 'NN'), ('villainth', 'JJ'), ('moment', 'NN'), ('starlord', 'NN'), ('scream', 'NN'), ('agoni', 'NN'), ('rocket', 'NN'), ('live', 'JJ'), ('moment', 'NN'), ('reson', 'NN'), ('lost', 'VBD'), ('mani', 'JJ'), ('peopl', 'NN'), ('close', 'JJ'), ('cant', 'JJ'), ('stand', 'NN'), ('lose', 'VB'), ('anyon', 'NN'), ('elsegamora', 'NN'), ('get', 'NN'), ('lot', 'NN'), ('charact', 'NN'), ('develop', 'VB'), ('im', 'JJ'), ('glad', 'NN'), ('didnt', 'NN'), ('kiss', 'NN'), ('quill', 'JJ'), ('end', 'NN'), ('would', 'MD'), ('felt', 'VB'), ('littl', 'JJ'), ('cheapdrax', 'NN'), ('manti', 'NN'), ('nebula', 'NN'), ('adam', 'NN'), ('warlock', 'NN'), ('other', 'JJ'), ('also', 'RB'), ('got', 'VBD'), ('moment', 'NN'), ('well', 'RB'), ('im', 'JJ'), ('surpris', 'NN'), ('nobodi', 'NN'), ('die', 'VBP'), ('honeston', 'NN'), ('problem', 'NN'), ('movi', 'NN'), ('humor', 'NN'), ('lot', 'NN'), ('time', 'NN'), ('good', 'JJ'), ('passabl', 'NN'), ('time', 'NN'), ('undercut', 'JJ'), ('realli', 'NN'), ('emot', 'NN'), ('scenesi', 'NN'), ('could', 'MD'), ('say', 'VB'), ('lot', 'NN'), ('review', 'VB'), ('alreadi', 'IN'), ('long', 'RB')]\n",
            "\n",
            "Named Entity Recognition:\n",
            "two: CARDINAL\n",
            "revealedchukwudi iwuji: ORG\n",
            "villainth: ORDINAL\n",
            "starlord: ORG\n",
            "anyon elsegamora: PERSON\n",
            "quill: PERSON\n",
            "\n",
            "POS Counts:\n",
            "NOUN: 0\n",
            "VERB: 0\n",
            "ADJ: 0\n",
            "ADV: 0\n",
            "\n",
            "Dependency Parsing Tree:\n",
            "guardian -- compound --> galaxi\n",
            "galaxi -- compound --> volum\n",
            "volum -- nmod --> ridicul\n",
            "chaotic -- amod --> ridicul\n",
            "weird -- amod --> ridicul\n",
            "oftentim -- compound --> ridicul\n",
            "ridicul -- nsubj --> say\n",
            "also -- advmod --> full\n",
            "full -- amod --> emot\n",
            "heart -- compound --> emot\n",
            "emot -- nmod --> themesi\n",
            "great -- amod --> themesi\n",
            "themesi -- nsubj --> say\n",
            "must -- aux --> say\n",
            "say -- ROOT --> say\n",
            "best -- amod --> marvel\n",
            "marvel -- compound --> endgam\n",
            "movi -- compound --> sinc\n",
            "sinc -- compound --> endgam\n",
            "endgam -- nsubj --> need\n",
            "that -- det --> necessarili\n",
            "necessarili -- nsubj --> need\n",
            "hard -- advmod --> necessarili\n",
            "though -- mark --> need\n",
            "need -- ccomp --> say\n",
            "surpass -- amod --> way\n",
            "way -- npadvmod --> home\n",
            "home -- advmod --> need\n",
            "amaz -- prep --> need\n",
            "high -- amod --> moment\n",
            "moment -- pobj --> amaz\n",
            "lazi -- compound --> need\n",
            "other -- amod --> desper\n",
            "marvel -- compound --> desper\n",
            "desper -- compound --> need\n",
            "need -- nsubj --> hit\n",
            "hit -- ccomp --> say\n",
            "theyv -- compound --> final\n",
            "final -- amod --> crew\n",
            "got -- compound --> crew\n",
            "ithighlightseveri -- compound --> crew\n",
            "member -- compound --> crew\n",
            "crew -- nsubj --> got\n",
            "got -- ccomp --> stood\n",
            "time -- npadvmod --> shine\n",
            "shine -- ccomp --> got\n",
            "rocket -- compound --> definit\n",
            "definit -- dobj --> shine\n",
            "one -- nsubj --> stood\n",
            "stood -- ccomp --> hit\n",
            "though -- mark --> see\n",
            "see -- advcl --> stood\n",
            "friend -- compound --> grief\n",
            "die -- compound --> wail\n",
            "wail -- compound --> grief\n",
            "pain -- compound --> grief\n",
            "grief -- dobj --> see\n",
            "made -- acl --> grief\n",
            "bawl -- nsubj --> rip\n",
            "high -- amod --> evolutionari\n",
            "evolutionari -- compound --> rip\n",
            "mock -- amod --> man\n",
            "man -- compound --> stuff\n",
            "heavi -- compound --> stuff\n",
            "stuff -- compound --> proce\n",
            "proce -- compound --> rip\n",
            "rip -- nsubj --> face\n",
            "face -- advcl --> stood\n",
            "two -- nummod --> friend\n",
            "friend -- dobj --> face\n",
            "shot -- advcl --> stood\n",
            "well -- advmod --> thu\n",
            "thu -- amod --> rocket\n",
            "rocket -- compound --> traumat\n",
            "traumat -- nsubj --> pastim\n",
            "pastim -- ccomp --> shot\n",
            "revealedchukwudi -- compound --> fantast\n",
            "iwuji -- compound --> fantast\n",
            "fantast -- nsubj --> villain\n",
            "villain -- conj --> shot\n",
            "certain -- amod --> point\n",
            "point -- dobj --> villain\n",
            "downright -- npadvmod --> villain\n",
            "terrifi -- compound --> realli\n",
            "realli -- dobj --> villain\n",
            "like -- prep --> realli\n",
            "line -- compound --> god\n",
            "god -- pobj --> like\n",
            "that -- nsubj --> took\n",
            "took -- relcl --> realli\n",
            "place -- dobj --> took\n",
            "convinc -- dobj --> took\n",
            "villainth -- compound --> moment\n",
            "moment -- nmod --> rocket\n",
            "starlord -- compound --> scream\n",
            "scream -- compound --> rocket\n",
            "agoni -- compound --> rocket\n",
            "rocket -- npadvmod --> shot\n",
            "live -- amod --> moment\n",
            "moment -- compound --> reson\n",
            "reson -- nsubj --> lost\n",
            "lost -- conj --> stood\n",
            "mani -- compound --> peopl\n",
            "peopl -- dobj --> lost\n",
            "close -- advmod --> lost\n",
            "ca -- aux --> stand\n",
            "nt -- neg --> stand\n",
            "stand -- advcl --> lost\n",
            "lose -- advcl --> stood\n",
            "anyon -- compound --> elsegamora\n",
            "elsegamora -- dobj --> lose\n",
            "get -- compound --> charact\n",
            "lot -- compound --> charact\n",
            "charact -- nsubj --> develop\n",
            "develop -- dep --> stood\n",
            "i -- dobj --> develop\n",
            "m -- ccomp --> develop\n",
            "glad -- acomp --> m\n",
            "did -- aux --> kiss\n",
            "nt -- neg --> kiss\n",
            "kiss -- xcomp --> glad\n",
            "quill -- compound --> end\n",
            "end -- dobj --> kiss\n",
            "would -- aux --> felt\n",
            "felt -- ccomp --> say\n",
            "littl -- compound --> cheapdrax\n",
            "cheapdrax -- compound --> manti\n",
            "manti -- nsubj --> warlock\n",
            "nebula -- compound --> adam\n",
            "adam -- appos --> manti\n",
            "warlock -- compound --> other\n",
            "other -- nsubj --> got\n",
            "also -- advmod --> got\n",
            "got -- ccomp --> felt\n",
            "moment -- dobj --> got\n",
            "well -- intj --> die\n",
            "i -- nsubj --> die\n",
            "m -- appos --> i\n",
            "surpris -- compound --> nobodi\n",
            "nobodi -- appos --> i\n",
            "die -- compound --> movi\n",
            "honeston -- compound --> problem\n",
            "problem -- compound --> movi\n",
            "movi -- compound --> humor\n",
            "humor -- compound --> lot\n",
            "lot -- compound --> time\n",
            "time -- npadvmod --> say\n",
            "good -- amod --> time\n",
            "passabl -- amod --> time\n",
            "time -- npadvmod --> say\n",
            "undercut -- amod --> scenesi\n",
            "realli -- compound --> emot\n",
            "emot -- compound --> scenesi\n",
            "scenesi -- nsubj --> say\n",
            "could -- aux --> say\n",
            "say -- ROOT --> say\n",
            "lot -- compound --> alreadi\n",
            "review -- compound --> alreadi\n",
            "alreadi -- dobj --> say\n",
            "long -- advmod --> say\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk import pos_tag, word_tokenize, RegexpParser\n",
        "import spacy\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Assuming df and column name are defined\n",
        "text = cleaned_df['Cleaned Review'].iloc[0]\n",
        "print(text)  # Replace 'Cleaned Review' with the actual column name\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Part-of-speech tagging using NLTK\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "# Print POS tags obtained using NLTK\n",
        "print(\"\\nPOS Tags (NLTK):\")\n",
        "print(pos_tags)\n",
        "\n",
        "# Named Entity Recognition using spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "\n",
        "# Print NER\n",
        "print(\"\\nNamed Entity Recognition:\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text}: {ent.label_}\")\n",
        "\n",
        "pos_counts = {'NOUN': 0, 'VERB': 0, 'ADJ': 0, 'ADV': 0}\n",
        "for _, pos in pos_tags:  # Fix the unpacking here\n",
        "    if pos in pos_counts:\n",
        "        pos_counts[pos] += 1\n",
        "\n",
        "# Print POS counts\n",
        "print(\"\\nPOS Counts:\")\n",
        "for pos, count in pos_counts.items():\n",
        "    print(f\"{pos}: {count}\")\n",
        "\n",
        "# Dependency Parsing using spaCy\n",
        "print(\"\\nDependency Parsing Tree:\")\n",
        "for token in doc:\n",
        "    print(f\"{token.text} -- {token.dep_} --> {token.head.text}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''In this sentence, the constituency parsing tree breaks down the grammatical structure and organization of the text into phrases and their syntactic relationships. For instance, it identifies noun phrases like \"the Guardian of the Galaxy volume\" and \"the story of family loss,\" as well as verb phrases like \"handled with care\" and \"help hammer home themes.\" Each node in the constituency parsing tree represents a phrase, and the tree structure illustrates how these phrases are nested within one another.\n",
        "\n",
        "On the other hand, the dependency parsing tree focuses on the grammatical dependencies between words, revealing how they relate to each other in terms of syntactic roles. For example, it identifies the subject-verb relationships like \"sat phase film,\" \"fail inspires guardian,\" and \"feel like a breath of fresh air.\" The tree illustrates the dependencies between words, emphasizing the core elements of the sentence and showcasing the connections between them. Together, constituency and dependency parsing provide a comprehensive understanding of the sentence's syntactic structure and the relationships between its constituent parts. '''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##The web-scraping is interesting but selecting the websites is challenging task few websites such as amazon required the code and permission to access their api if possible could you please let us know how can we overcome to access specific websites. Constituency Parsing and Dependency Parsing is also a tough is also challenging.Remaining everything is easy and time provided for the assignment is also sufficient.\n"
      ],
      "metadata": {
        "id": "_e557s2w4BpK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}